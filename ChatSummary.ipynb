{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents found\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "db = Chroma(collection_name=\"Docs\", embedding_function=embedder)\n",
    "\n",
    "ids = db.get()['ids']\n",
    "if ids:\n",
    "    db.delete(ids)\n",
    "    print(f\"Deleted {len(ids)} documents\")\n",
    "else:\n",
    "    print(\"No documents found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 15:18:44,425 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Device set to use cpu\n",
      "llama_new_context_with_model: n_ctx_per_seq (1024) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Ready\n",
      "\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "2024-12-22 15:19:08,953 - INFO - Indexed 203 documents from sample3.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed successfully.\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.41it/s]\n",
      "/tmp/ipykernel_15301/1133244675.py:173: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return self.llm(prompt)\n",
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context, the answer to question 2 is:\n",
      "\n",
      "Programmer shall have no later than 30 days from the respective date that Programmer grants any additional non-economic terms to Other Distributor.\n",
      "Answer:   Based on the provided context, the answer to question 2 is:\n",
      "\n",
      "Programmer shall have no later than 30 days from the respective date that Programmer grants any additional non-economic terms to Other Distributor.\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 113.57it/s]\n",
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the given information, here are the service charges for a period of 3 months:\n",
      "\n",
      "Month 1:\n",
      "\n",
      "* Average number of Service Subscribers: .1 (as per the agreement)\n",
      "* Number of Bulk-Bill Service Subscribers: x (to be determined based on the actual number of units subject to the Bulk-Bill Arrangement)\n",
      "* Monthly retail rate XYZ charges per unit for programming only: $X\n",
      "* Quotient obtained by dividing the monthly retail rate by the number of Service Subscribers: $Y\n",
      "* License Fee payable by XYZ for Month 1: $0.1 x ($X / Y)\n",
      "\n",
      "Month 2:\n",
      "\n",
      "* Average number of Service Subscribers: .1 (as per the agreement)\n",
      "* Number of Bulk-Bill Service Subscribers: x (to be determined based on the actual number of units subject to the Bulk-Bill Arrangement)\n",
      "* Monthly retail rate XYZ charges per unit for programming only: $X\n",
      "* Quotient obtained by dividing the monthly retail rate by the number\n",
      "Answer:   Based on the given information, here are the service charges for a period of 3 months:\n",
      "\n",
      "Month 1:\n",
      "\n",
      "* Average number of Service Subscribers: .1 (as per the agreement)\n",
      "* Number of Bulk-Bill Service Subscribers: x (to be determined based on the actual number of units subject to the Bulk-Bill Arrangement)\n",
      "* Monthly retail rate XYZ charges per unit for programming only: $X\n",
      "* Quotient obtained by dividing the monthly retail rate by the number of Service Subscribers: $Y\n",
      "* License Fee payable by XYZ for Month 1: $0.1 x ($X / Y)\n",
      "\n",
      "Month 2:\n",
      "\n",
      "* Average number of Service Subscribers: .1 (as per the agreement)\n",
      "* Number of Bulk-Bill Service Subscribers: x (to be determined based on the actual number of units subject to the Bulk-Bill Arrangement)\n",
      "* Monthly retail rate XYZ charges per unit for programming only: $X\n",
      "* Quotient obtained by dividing the monthly retail rate by the number\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.59it/s]\n",
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Based on the given text, here are two parties involved in the agreement:\n",
      "\n",
      "1. XYZ\n",
      "2. Programmer\n",
      "Answer:   Sure! Based on the given text, here are two parties involved in the agreement:\n",
      "\n",
      "1. XYZ\n",
      "2. Programmer\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n",
      "Invalid choice. Please try again.\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 99.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.05it/s]\n",
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the given text, XYZ is a party to the agreement. Specifically, it is mentioned that XYZ acknowledges receiving two receivers and decoders (IRDs) from the Programmer, and later in the text, XYZ grants the non-exclusive right and license to receive and distribute each linear Service, including all feeds and versions of each Service in the Territory via the Hi-Tech Distribution System to Subscribers during the Term. Therefore, XYZ is the company or entity that is party to the agreement with the Programmer.\n",
      "Answer:   Based on the given text, XYZ is a party to the agreement. Specifically, it is mentioned that XYZ acknowledges receiving two receivers and decoders (IRDs) from the Programmer, and later in the text, XYZ grants the non-exclusive right and license to receive and distribute each linear Service, including all feeds and versions of each Service in the Territory via the Hi-Tech Distribution System to Subscribers during the Term. Therefore, XYZ is the company or entity that is party to the agreement with the Programmer.\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n",
      "Invalid choice. Please try again.\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.23it/s]\n",
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context, XYZ is a company.\n",
      "Answer:   Based on the provided context, XYZ is a company.\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "class Settings:\n",
    "    def __init__(self):\n",
    "        self.embedder = \"BAAI/bge-small-en-v1.5\"\n",
    "        self.ranker = \"cross-encoder/ms-marco-TinyBERT-L-2\"\n",
    "        self.model = \"models/Llama-2-7b-chat-hf.Q8_0.gguf\"\n",
    "        self.gpuDepth = 0\n",
    "        self.threads = min(12, psutil.cpu_count(logical=False))\n",
    "        self.batch = 512\n",
    "        self.chunk = 384\n",
    "        self.overlap = 64\n",
    "        self.context = 1024\n",
    "        self.temp = 0.7\n",
    "        self.topk = 3\n",
    "        self.retrieve = 8\n",
    "        self.rerank = 3\n",
    "        self.db = \"Docs\"\n",
    "        self.fusion = 60\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=cfg.chunk,\n",
    "            chunk_overlap=cfg.overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        self.loaders = {'.pdf': PyPDFLoader, '.docx': Docx2txtLoader, '.txt': TextLoader}\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\.,!?;:-]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def load(self, path):\n",
    "        ext = Path(path).suffix.lower()\n",
    "        loader = self.loaders.get(ext)\n",
    "        if not loader:\n",
    "            raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "        docs = loader(str(path)).load()\n",
    "        return [self.clean(doc.page_content) for doc in docs]\n",
    "\n",
    "    def split(self, docs):\n",
    "        text = \" \".join(docs)\n",
    "        chunks = self.splitter.split_text(text)\n",
    "        return [chunk for chunk in chunks if len(chunk) >= 50]\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model):\n",
    "        self.model = SentenceTransformer(model, device=\"cpu\")\n",
    "\n",
    "    def embed(self, texts):\n",
    "        batch = 32\n",
    "        vectors = []\n",
    "        for i in range(0, len(texts), batch):\n",
    "            chunk = texts[i:i + batch]\n",
    "            vectors.extend(self.model.encode(chunk, normalize_embeddings=True))\n",
    "        return vectors\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.embed(texts)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.model.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "class RagFusion:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "    def merge(self, rankings):\n",
    "        scores = {}\n",
    "        for ranking in rankings:\n",
    "            for rank, item in enumerate(ranking):\n",
    "                if isinstance(item, tuple):\n",
    "                    doc, _ = item\n",
    "                    content = doc.page_content\n",
    "                else:\n",
    "                    content = item.page_content\n",
    "                \n",
    "                if content not in scores:\n",
    "                    scores[content] = 0\n",
    "                scores[content] += 1.0 / (rank + self.cfg.fusion)\n",
    "        \n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.log = logging.getLogger(__name__)\n",
    "        self.embedder = Embedder(cfg.embedder)\n",
    "        self.ranker = pipeline(\"text-classification\", model=cfg.ranker, device=-1)\n",
    "        self.store = Chroma(collection_name=cfg.db, embedding_function=self.embedder)\n",
    "        self.fusion = RagFusion(cfg)\n",
    "        \n",
    "        callback = StreamingStdOutCallbackHandler()\n",
    "        self.llm = LlamaCpp(\n",
    "            model_path=cfg.model,\n",
    "            n_gpu_layers=cfg.gpuDepth,\n",
    "            n_threads=cfg.threads,\n",
    "            n_batch=cfg.batch,\n",
    "            n_ctx=cfg.context,\n",
    "            callbacks=[callback],\n",
    "            verbose=False,\n",
    "            temperature=cfg.temp,\n",
    "            top_k=cfg.topk,\n",
    "        )\n",
    "\n",
    "    def index(self, texts, source):\n",
    "        try:\n",
    "            vectors = self.embedder.embed_documents(texts)\n",
    "            self.store.add_texts(texts, metadatas=[{\"source\": source}] * len(texts), embeddings=vectors)\n",
    "            self.log.info(f\"Indexed {len(texts)} documents from {source}\")\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Indexing error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def semantic(self, query):\n",
    "        try:\n",
    "            results = self.store.similarity_search_with_score(query, k=self.cfg.retrieve)\n",
    "            return [(doc, score) for doc, score in results]\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Search error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def keyword(self, query):\n",
    "        try:\n",
    "            docs = self.store.similarity_search(query, k=self.cfg.retrieve)\n",
    "            scores = []\n",
    "            terms = set(query.lower().split())\n",
    "            \n",
    "            for doc in docs:\n",
    "                text = doc.page_content.lower()\n",
    "                score = sum(1 for term in terms if term in text) / len(terms)\n",
    "                scores.append((doc, score))\n",
    "            \n",
    "            return sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Keyword error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def ask(self, query):\n",
    "        try:\n",
    "            semResults = self.semantic(query)\n",
    "            keyResults = self.keyword(query)\n",
    "            \n",
    "            if not semResults or not keyResults:\n",
    "                raise ValueError(\"No results found\")\n",
    "                \n",
    "            merged = self.fusion.merge([semResults, keyResults])\n",
    "            \n",
    "            docs = []\n",
    "            seen = set()\n",
    "            for content, _ in merged:\n",
    "                if len(docs) >= self.cfg.rerank:\n",
    "                    break\n",
    "                if content not in seen:\n",
    "                    docs.append(content)\n",
    "                    seen.add(content)\n",
    "            \n",
    "            context = \" \".join(docs[:self.cfg.rerank])\n",
    "            prompt = f\"<s>[INST] Context:\\n{context}\\n\\nQuestion:\\n{query}[/INST]\"\n",
    "            return self.llm(prompt)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Query error: {str(e)}\")\n",
    "            return \"Error processing query\"\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, \n",
    "                       format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    try:\n",
    "        cfg = Settings()\n",
    "        proc = TextProcessor(cfg)\n",
    "        brain = Brain(cfg)\n",
    "\n",
    "        print(\"System Ready\\n\")\n",
    "        while True:\n",
    "            print(\"1. Add Docs\\n2. Ask Question\\n3. Exit\")\n",
    "            choice = input(\"Choice: \")\n",
    "            \n",
    "            try:\n",
    "                if choice == \"1\":\n",
    "                    path = input(\"Doc Path: \")\n",
    "                    docs = proc.load(path)\n",
    "                    chunks = proc.split(docs)\n",
    "                    brain.index(chunks, source=path)\n",
    "                    print(\"Indexed successfully.\")\n",
    "                    \n",
    "                elif choice == \"2\":\n",
    "                    query = input(\"Question: \")\n",
    "                    answer = brain.ask(query)\n",
    "                    print(\"\\nAnswer:\", answer)\n",
    "                    \n",
    "                elif choice == \"3\":\n",
    "                    print(\"Exiting...\")\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Invalid choice. Please try again.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error: {str(e)}\")\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {str(e)}\")\n",
    "        print(f\"Fatal error occurred: {str(e)}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22644/202629904.py:81: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embed = HuggingFaceEmbeddings(\n",
      "2024-12-22 18:55:32,956 - INFO - Load pretrained SentenceTransformer: intfloat/e5-base-v2\n",
      "Device set to use cpu\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Ready\n",
      "\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: failed to munlock buffer: Cannot allocate memory\n",
      "2024-12-22 18:55:54,977 - INFO - Saved 203 from sample3.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added successfully\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 18:56:23,070 - ERROR - Error: Cannot load: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Cannot load: \n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 18:56:46,425 - INFO - Saved 203 from sample3.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added successfully\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22644/202629904.py:138: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  ans = self.llm(prompt)\n",
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the chat history provided, the name of the distributor is:\n",
      "\n",
      "Other Distributor\n",
      "Answer:   Based on the chat history provided, the name of the distributor is:\n",
      "\n",
      "Other Distributor\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the chat history provided, the name of the distributor is \"Other Distributor\".\n",
      "\n",
      "To answer your question, we need to know which specific aspect of the service subscriber charges you would like to know. Here are some options:\n",
      "\n",
      "1. A La Carte Rates: The a la carte rates for GEC1 and MusicChannel1 are as follows:\n",
      "\t* For GEC1: US$7.495 per Service Subscriber per month, or 50% of the Net Retail Price paid by each Service Subscriber for the relevant month.\n",
      "\t* For MusicChannel1: US$7.495 per Service Subscriber per month, or 50% of the Net Retail Price paid by each Service Subscriber for the relevant month.\n",
      "2. Package Rates: The rates for packages comprising of the service are as follows:\n",
      "\t* For GEC1 and MusicChannel1, the rate is US$7.495 per Service Subscriber per month.\n",
      "3. Number of Service Subscribers: At the beginning of the relevant month, there were x number of Service Subscribers who had subscribed to the service\n",
      "Answer:   Based on the chat history provided, the name of the distributor is \"Other Distributor\".\n",
      "\n",
      "To answer your question, we need to know which specific aspect of the service subscriber charges you would like to know. Here are some options:\n",
      "\n",
      "1. A La Carte Rates: The a la carte rates for GEC1 and MusicChannel1 are as follows:\n",
      "\t* For GEC1: US$7.495 per Service Subscriber per month, or 50% of the Net Retail Price paid by each Service Subscriber for the relevant month.\n",
      "\t* For MusicChannel1: US$7.495 per Service Subscriber per month, or 50% of the Net Retail Price paid by each Service Subscriber for the relevant month.\n",
      "2. Package Rates: The rates for packages comprising of the service are as follows:\n",
      "\t* For GEC1 and MusicChannel1, the rate is US$7.495 per Service Subscriber per month.\n",
      "3. Number of Service Subscribers: At the beginning of the relevant month, there were x number of Service Subscribers who had subscribed to the service\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.embed = \"intfloat/e5-base-v2\"\n",
    "        self.rank = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "        self.model = \"models/llama-2-13b-chat.Q4_K_M.gguf\"\n",
    "        self.gpu = 0\n",
    "        self.cpu = min(8, psutil.cpu_count(logical=False))\n",
    "        self.batch = 32\n",
    "        self.chunk = 384\n",
    "        self.overlap = 64\n",
    "        self.ctx = 2048\n",
    "        self.temp = 0.1\n",
    "        self.top = 5\n",
    "        self.get = 10\n",
    "        self.best = 3\n",
    "        self.db = \"Knowledge\"\n",
    "        self.blend = 40\n",
    "        self.hist = 8\n",
    "\n",
    "class DocProc:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.split = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=cfg.chunk,\n",
    "            chunk_overlap=cfg.overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        self.load = {'.pdf': PyPDFLoader, '.docx': Docx2txtLoader, '.txt': TextLoader}\n",
    "\n",
    "    def clean(self, txt):\n",
    "        txt = re.sub(r'\\s+', ' ', txt)\n",
    "        txt = re.sub(r'[^\\w\\s\\.,!?;:-]', '', txt)\n",
    "        return txt.strip()\n",
    "\n",
    "    def read(self, path):\n",
    "        ext = Path(path).suffix.lower()\n",
    "        if ext not in self.load:\n",
    "            raise ValueError(f\"Cannot load: {ext}\")\n",
    "        docs = self.load[ext](str(path)).load()\n",
    "        return [self.clean(d.page_content) for d in docs]\n",
    "\n",
    "    def chunk(self, docs):\n",
    "        txt = \" \".join(docs)\n",
    "        parts = self.split.split_text(txt)\n",
    "        return [p for p in parts if len(p) >= 50]\n",
    "\n",
    "class ChatMem:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.hist = []\n",
    "\n",
    "    def add(self, msg):\n",
    "        self.hist.append(msg)\n",
    "        if len(self.hist) > self.size:\n",
    "            self.hist.pop(0)\n",
    "\n",
    "    def get(self):\n",
    "        return self.hist\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.log = logging.getLogger(__name__)\n",
    "        \n",
    "        model_kwargs = {'device': 'cpu'}\n",
    "        encode_kwargs = {'normalize_embeddings': True, 'batch_size': 32}\n",
    "        self.embed = HuggingFaceEmbeddings(\n",
    "            model_name=cfg.embed,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        \n",
    "        self.rank = pipeline(\"text-classification\", model=cfg.rank, device=-1)\n",
    "        self.store = Chroma(\n",
    "            collection_name=cfg.db,\n",
    "            embedding_function=self.embed\n",
    "        )\n",
    "        self.mem = ChatMem(cfg.hist)\n",
    "        \n",
    "        self.llm = LlamaCpp(\n",
    "            model_path=cfg.model,\n",
    "            n_gpu_layers=cfg.gpu,\n",
    "            n_threads=cfg.cpu,\n",
    "            n_batch=cfg.batch,\n",
    "            n_ctx=cfg.ctx,\n",
    "            callbacks=[StreamingStdOutCallbackHandler()],\n",
    "            verbose=False,\n",
    "            temperature=cfg.temp,\n",
    "            top_k=cfg.top,\n",
    "            use_mmap=True,\n",
    "            use_mlock=True\n",
    "        )\n",
    "\n",
    "    def save(self, txts, src):\n",
    "        try:\n",
    "            self.store.add_texts(txts, metadatas=[{\"src\": src}] * len(txts))\n",
    "            self.log.info(f\"Saved {len(txts)} from {src}\")\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Save failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def deep(self, q):\n",
    "        try:\n",
    "            hits = self.store.similarity_search_with_score(q, k=self.cfg.get)\n",
    "            pairs = []\n",
    "            for doc, score in hits:\n",
    "                rank = self.rank([q, doc.page_content])[0]\n",
    "                pairs.append((doc.page_content, float(rank[\"score\"])))\n",
    "            return sorted(pairs, key=lambda x: x[1], reverse=True)\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Deep search failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def ask(self, q):\n",
    "        try:\n",
    "            hits = self.deep(q)\n",
    "            if not hits:\n",
    "                return \"No relevant information found\"\n",
    "\n",
    "            ctx = \"\\n\".join([d[0] for d in hits[:self.cfg.best]])\n",
    "            hist = \"\\n\".join(self.mem.get())\n",
    "            \n",
    "            prompt = f\"<s>[INST] Chat History:\\n{hist}\\n\\nContext:\\n{ctx}\\n\\nQuestion:\\n{q}[/INST]\"\n",
    "            ans = self.llm(prompt)\n",
    "            \n",
    "            self.mem.add(f\"User: {q}\\nAssistant: {ans}\")\n",
    "            return ans\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Query failed: {str(e)}\")\n",
    "            return \"Error processing query\"\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, \n",
    "                       format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    try:\n",
    "        cfg = Config()\n",
    "        proc = DocProc(cfg)\n",
    "        brain = Brain(cfg)\n",
    "\n",
    "        print(\"System Ready\\n\")\n",
    "        while True:\n",
    "            print(\"1. Add Docs\\n2. Ask Question\\n3. Exit\")\n",
    "            pick = input(\"Choice: \")\n",
    "            \n",
    "            try:\n",
    "                if pick == \"1\":\n",
    "                    path = input(\"Doc Path: \")\n",
    "                    docs = proc.read(path)\n",
    "                    parts = proc.chunk(docs)\n",
    "                    brain.save(parts, path)\n",
    "                    print(\"Added successfully\")\n",
    "                    \n",
    "                elif pick == \"2\":\n",
    "                    q = input(\"Question: \")\n",
    "                    ans = brain.ask(q)\n",
    "                    print(\"\\nAnswer:\", ans)\n",
    "                    \n",
    "                elif pick == \"3\":\n",
    "                    print(\"Exiting...\")\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Invalid choice\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error: {str(e)}\")\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal: {str(e)}\")\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
