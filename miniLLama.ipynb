{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 14:36:21,439 - INFO - Use pytorch device_name: cpu\n",
      "2024-12-21 14:36:21,439 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents found\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "db = Chroma(collection_name=\"Docs\", embedding_function=embedder)\n",
    "\n",
    "ids = db.get()['ids']\n",
    "if ids:\n",
    "    db.delete(ids)\n",
    "    print(f\"Deleted {len(ids)} documents\")\n",
    "else:\n",
    "    print(\"No documents found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 14:36:25,942 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Device set to use cpu\n",
      "llama_new_context_with_model: n_ctx_per_seq (1024) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Ready\n",
      "\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\n",
      "2024-12-21 14:36:44,853 - INFO - Indexed 203 documents from sample3.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed successfully.\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.99it/s]\n",
      "/tmp/ipykernel_27485/2615144059.py:173: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return self.llm(prompt)\n",
      "/home/shubham/DATA/Projects/vlm/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for the Service, and how many Service Subscribers have subscribed to the Service at the beginning and end of each month? Answer:\n",
      "\n",
      "The Service Subscriber Charges per month [/INST] for the Service are as follows:\n",
      "1. A La Carte Rates: a For GEC1 greater of: i US7.495 per Service Subscriber per month, or ii 50 of the Net Retail Price paid by each Service Subscriber for the relevant month. The number of Service Subscribers who have subscribed to the Service at the beginning and end of a month is as follows:\n",
      "1. For MusicChannel1 greater of: i US7.495 per Service Subscriber per month, or ii 50 of the Net Retail Price paid by each Service Subscriber for the relevant month . 2. For GEC1 greater of: i US7.495 per Service Subscribe r per month, or ii 50 of the Net Retail Price paid by each Service Subscriber for the relevant month .\n",
      "The number of Service Subscribers who have subscribed to the Service at the beginning and end of each calendar year is also as\n",
      "Answer:  for the Service, and how many Service Subscribers have subscribed to the Service at the beginning and end of each month? Answer:\n",
      "\n",
      "The Service Subscriber Charges per month [/INST] for the Service are as follows:\n",
      "1. A La Carte Rates: a For GEC1 greater of: i US7.495 per Service Subscriber per month, or ii 50 of the Net Retail Price paid by each Service Subscriber for the relevant month. The number of Service Subscribers who have subscribed to the Service at the beginning and end of a month is as follows:\n",
      "1. For MusicChannel1 greater of: i US7.495 per Service Subscriber per month, or ii 50 of the Net Retail Price paid by each Service Subscriber for the relevant month . 2. For GEC1 greater of: i US7.495 per Service Subscribe r per month, or ii 50 of the Net Retail Price paid by each Service Subscriber for the relevant month .\n",
      "The number of Service Subscribers who have subscribed to the Service at the beginning and end of each calendar year is also as\n",
      "1. Add Docs\n",
      "2. Ask Question\n",
      "3. Exit\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "class Settings:\n",
    "    def __init__(self):\n",
    "        self.embedder = \"BAAI/bge-small-en-v1.5\"\n",
    "        self.ranker = \"cross-encoder/ms-marco-TinyBERT-L-2\"\n",
    "        self.model = \"models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "        self.gpuDepth = 0\n",
    "        self.threads = min(12, psutil.cpu_count(logical=False))\n",
    "        self.batch = 512\n",
    "        self.chunk = 384\n",
    "        self.overlap = 64\n",
    "        self.context = 1024\n",
    "        self.temp = 0.7\n",
    "        self.topk = 3\n",
    "        self.retrieve = 8\n",
    "        self.rerank = 3\n",
    "        self.db = \"Docs\"\n",
    "        self.fusion = 60\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=cfg.chunk,\n",
    "            chunk_overlap=cfg.overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        self.loaders = {'.pdf': PyPDFLoader, '.docx': Docx2txtLoader, '.txt': TextLoader}\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\.,!?;:-]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def load(self, path):\n",
    "        ext = Path(path).suffix.lower()\n",
    "        loader = self.loaders.get(ext)\n",
    "        if not loader:\n",
    "            raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "        docs = loader(str(path)).load()\n",
    "        return [self.clean(doc.page_content) for doc in docs]\n",
    "\n",
    "    def split(self, docs):\n",
    "        text = \" \".join(docs)\n",
    "        chunks = self.splitter.split_text(text)\n",
    "        return [chunk for chunk in chunks if len(chunk) >= 50]\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model):\n",
    "        self.model = SentenceTransformer(model, device=\"cpu\")\n",
    "\n",
    "    def embed(self, texts):\n",
    "        batch = 32\n",
    "        vectors = []\n",
    "        for i in range(0, len(texts), batch):\n",
    "            chunk = texts[i:i + batch]\n",
    "            vectors.extend(self.model.encode(chunk, normalize_embeddings=True))\n",
    "        return vectors\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.embed(texts)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.model.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "class RagFusion:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "    def merge(self, rankings):\n",
    "        scores = {}\n",
    "        for ranking in rankings:\n",
    "            for rank, item in enumerate(ranking):\n",
    "                if isinstance(item, tuple):\n",
    "                    doc, _ = item\n",
    "                    content = doc.page_content\n",
    "                else:\n",
    "                    content = item.page_content\n",
    "                \n",
    "                if content not in scores:\n",
    "                    scores[content] = 0\n",
    "                scores[content] += 1.0 / (rank + self.cfg.fusion)\n",
    "        \n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.log = logging.getLogger(__name__)\n",
    "        self.embedder = Embedder(cfg.embedder)\n",
    "        self.ranker = pipeline(\"text-classification\", model=cfg.ranker, device=-1)\n",
    "        self.store = Chroma(collection_name=cfg.db, embedding_function=self.embedder)\n",
    "        self.fusion = RagFusion(cfg)\n",
    "        \n",
    "        callback = StreamingStdOutCallbackHandler()\n",
    "        self.llm = LlamaCpp(\n",
    "            model_path=cfg.model,\n",
    "            n_gpu_layers=cfg.gpuDepth,\n",
    "            n_threads=cfg.threads,\n",
    "            n_batch=cfg.batch,\n",
    "            n_ctx=cfg.context,\n",
    "            callbacks=[callback],\n",
    "            verbose=False,\n",
    "            temperature=cfg.temp,\n",
    "            top_k=cfg.topk,\n",
    "        )\n",
    "\n",
    "    def index(self, texts, source):\n",
    "        try:\n",
    "            vectors = self.embedder.embed_documents(texts)\n",
    "            self.store.add_texts(texts, metadatas=[{\"source\": source}] * len(texts), embeddings=vectors)\n",
    "            self.log.info(f\"Indexed {len(texts)} documents from {source}\")\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Indexing error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def semantic(self, query):\n",
    "        try:\n",
    "            results = self.store.similarity_search_with_score(query, k=self.cfg.retrieve)\n",
    "            return [(doc, score) for doc, score in results]\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Search error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def keyword(self, query):\n",
    "        try:\n",
    "            docs = self.store.similarity_search(query, k=self.cfg.retrieve)\n",
    "            scores = []\n",
    "            terms = set(query.lower().split())\n",
    "            \n",
    "            for doc in docs:\n",
    "                text = doc.page_content.lower()\n",
    "                score = sum(1 for term in terms if term in text) / len(terms)\n",
    "                scores.append((doc, score))\n",
    "            \n",
    "            return sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Keyword error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def ask(self, query):\n",
    "        try:\n",
    "            semResults = self.semantic(query)\n",
    "            keyResults = self.keyword(query)\n",
    "            \n",
    "            if not semResults or not keyResults:\n",
    "                raise ValueError(\"No results found\")\n",
    "                \n",
    "            merged = self.fusion.merge([semResults, keyResults])\n",
    "            \n",
    "            docs = []\n",
    "            seen = set()\n",
    "            for content, _ in merged:\n",
    "                if len(docs) >= self.cfg.rerank:\n",
    "                    break\n",
    "                if content not in seen:\n",
    "                    docs.append(content)\n",
    "                    seen.add(content)\n",
    "            \n",
    "            context = \" \".join(docs[:self.cfg.rerank])\n",
    "            prompt = f\"<s>[INST] Context:\\n{context}\\n\\nQuestion:\\n{query}[/INST]\"\n",
    "            return self.llm(prompt)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log.error(f\"Query error: {str(e)}\")\n",
    "            return \"Error processing query\"\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, \n",
    "                       format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    try:\n",
    "        cfg = Settings()\n",
    "        proc = TextProcessor(cfg)\n",
    "        brain = Brain(cfg)\n",
    "\n",
    "        print(\"System Ready\\n\")\n",
    "        while True:\n",
    "            print(\"1. Add Docs\\n2. Ask Question\\n3. Exit\")\n",
    "            choice = input(\"Choice: \")\n",
    "            \n",
    "            try:\n",
    "                if choice == \"1\":\n",
    "                    path = input(\"Doc Path: \")\n",
    "                    docs = proc.load(path)\n",
    "                    chunks = proc.split(docs)\n",
    "                    brain.index(chunks, source=path)\n",
    "                    print(\"Indexed successfully.\")\n",
    "                    \n",
    "                elif choice == \"2\":\n",
    "                    query = input(\"Question: \")\n",
    "                    answer = brain.ask(query)\n",
    "                    print(\"\\nAnswer:\", answer)\n",
    "                    \n",
    "                elif choice == \"3\":\n",
    "                    print(\"Exiting...\")\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Invalid choice. Please try again.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error: {str(e)}\")\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {str(e)}\")\n",
    "        print(f\"Fatal error occurred: {str(e)}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
