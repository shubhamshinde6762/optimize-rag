{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/DATA/Projects/VLM/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SentenceTransformerWrapper(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, docs: list[str]) -> list[list[float]]:\n",
    "        return self.model.encode(docs, normalize_embeddings=True).tolist()\n",
    "\n",
    "    def embed_query(self, query: str) -> list[float]:\n",
    "        return self.model.encode(query, normalize_embeddings=True).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 23:31:01,311 - INFO - Use pytorch device_name: cpu\n",
      "2024-12-17 23:31:01,311 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 102.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: question: What is the quotient obtained by dividing the Net Retail Price paid by each Service Subscriber per month?\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    TextLoader,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "class RAGFusionProcessor:\n",
    "    DEFAULTS = {\n",
    "        'size': 512,\n",
    "        'overlap': 50,\n",
    "        'model': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'name': \"Docs\",\n",
    "        'level': logging.INFO,\n",
    "        'depth': 10,\n",
    "        'factor': 0.5,\n",
    "        't5_model': '/home/shubham/.cache/huggingface/hub/models--mrm8488--t5-base-finetuned-question-generation-ap/snapshots/c81cbaf0ec96cc3623719e3d8b0f238da5456ca8',  # T5 model to be used for summarization or question-answering\n",
    "    }\n",
    "\n",
    "    LOADERS = {\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.docx': Docx2txtLoader,\n",
    "        '.txt': TextLoader\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.cfg = {**self.DEFAULTS, **kwargs}\n",
    "        logging.basicConfig(level=self.cfg['level'], format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.log = logging.getLogger(__name__)\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.cfg['size'],\n",
    "            chunk_overlap=self.cfg['overlap']\n",
    "        )\n",
    "        self.wrapper = SentenceTransformerWrapper(self.cfg['model'])\n",
    "        self.model = self.wrapper.model\n",
    "        self.store = Chroma(\n",
    "            collection_name=self.cfg['name'],\n",
    "            embedding_function=self.wrapper\n",
    "        )\n",
    "        ids = self.store._collection.get()['ids']\n",
    "        if ids:\n",
    "            self.store._collection.delete(ids)\n",
    "\n",
    "        # Initialize the T5 model and tokenizer\n",
    "        self.t5_tokenizer = T5Tokenizer.from_pretrained(self.cfg['t5_model'])\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(self.cfg['t5_model'])\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, typ, val, tb):\n",
    "        self.store = None\n",
    "\n",
    "    @staticmethod\n",
    "    def clean(text: str) -> str:\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s\\.,]', '', text)).strip()\n",
    "\n",
    "    def load(self, path: str) -> List[str]:\n",
    "        path = Path(path)\n",
    "        loader = self.LOADERS.get(path.suffix.lower())\n",
    "        docs = loader(str(path)).load()\n",
    "        return [self.clean(doc.page_content) for doc in docs]\n",
    "\n",
    "    def process(self, docs: List[str]) -> List[str]:\n",
    "        clean_docs = [self.clean(doc) for doc in docs]\n",
    "        return self.splitter.split_text(\" \".join(clean_docs))\n",
    "\n",
    "    def embed(self, chunks: List[str], src: str = \"default\") -> None:\n",
    "        docs = [Document(page_content=chunk, metadata={\"src\": src}) for chunk in chunks]\n",
    "        self.store.add_documents(docs)\n",
    "        print(f\"Total records: {self.store._collection.count()}\")\n",
    "\n",
    "    def search(self, qry: str, lim: int = 5) -> List[Tuple[Document, float]]:\n",
    "        results = self.store.max_marginal_relevance_search(\n",
    "            qry, k=lim, fetch_k=self.cfg['depth'], lambda_mult=self.cfg['factor']\n",
    "        )\n",
    "        return [(doc, 0.0) for doc in results]\n",
    "\n",
    "    def rank(self, qry: str, docs: List[Document]) -> List[Tuple[Document, float]]:\n",
    "        embed_qry = self.model.encode(qry, normalize_embeddings=True)\n",
    "        scores = [\n",
    "            np.dot(embed_qry, self.model.encode(doc.page_content, normalize_embeddings=True))\n",
    "            for doc in docs\n",
    "        ]\n",
    "        ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for doc, score in ranked[:5]: \n",
    "            summary = self.generate_summary(doc.page_content)\n",
    "            print(f\"Summary of chunk from {doc.metadata.get('src')}: {summary}\")\n",
    "\n",
    "        return ranked\n",
    "\n",
    "    def generate_summary(self, text: str) -> str:\n",
    "        inputs = self.t5_tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = self.t5_model.generate(inputs, max_length=150, num_beams=4, early_stopping=True)\n",
    "        summary = self.t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "\n",
    "def main():\n",
    "    import os\n",
    "    os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
    "\n",
    "    with RAGFusionProcessor() as proc:\n",
    "        path = \"sample3.pdf\"\n",
    "        raw_docs = proc.load(path)\n",
    "        chunks = proc.process(raw_docs)\n",
    "        proc.embed(chunks, src=path)\n",
    "        qry = \"tell me Service Subscriber charges\"\n",
    "        results = proc.search(qry)\n",
    "        ranked = proc.rank(qry, [doc for doc, _ in results])\n",
    "\n",
    "        for doc, score in ranked:\n",
    "            print(f\"Chunk: {doc.page_content}, Source: {doc.metadata.get('src')}, Score: {score}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
